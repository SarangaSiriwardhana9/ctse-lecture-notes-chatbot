{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd218b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\saranga\\desktop\\chatbot\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install basic packages - these should work without errors\n",
    "%pip install requests sentence-transformers\n",
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0beea886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n",
      "Split into 18 chunks\n",
      "Loading embedding model...\n",
      "Creating embeddings for chunks...\n",
      "\n",
      "============================================================\n",
      "  CTSE Lecture Notes Chatbot\n",
      "  Type 'exit' to quit or 'new' for a new question\n",
      "============================================================\n",
      "\n",
      "Question: What is an Artificial Neural Network ?\n",
      "--------------------------------------------------\n",
      "Answer: A Collection of Perceptron. A Layer can be made by stacking a set of Perceptron to get the outputs from the previous layer or Inputs. A network is a set of layers that are arranged in a organized manner (Sequential).\n",
      "\n",
      "\n",
      "Sources:\n",
      "Source 1 from ML Lec 2 - Part 1.pdf:\n",
      "SE4010 | Current Trends in SE| Introduction to Artificial Neural Networks| Jeewaka PereraGain an understanding of the structure and background of ANN ...\n",
      "\n",
      "Source 2 from ML Lec 2 - Part 1.pdf:\n",
      "rges SE4010 | Current Trends in SE| Introduction to Artificial Neural Networks| Jeewaka Perera SE4010 | Current Trends in SE| Introduction to Artifici...\n",
      "\n",
      "Source 3 from ML Lec 2 - Part 1.pdf:\n",
      " y=ቊ0,𝑥𝑃⋅𝑤1+𝑥𝑏⋅𝑤2+𝑥𝑟⋅𝑤3<𝑡 1,𝑥𝑃⋅𝑤1+𝑥𝑏⋅𝑤2+𝑥𝑟⋅𝑤3≥𝑡 •Its easier to compute with vectors y=ቊ0,𝑥⋅𝑤<𝑡 1,𝑥⋅𝑤≥𝑡 y=ቊ0,𝑥⋅𝑤+𝑏<0 1,𝑥⋅𝑤+𝑏≥0 SE4010 | Current Trends ...\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Type your next question, 'exit' to quit, or press Enter to continue\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 188\u001b[0m\n\u001b[0;32m    185\u001b[0m model, embeddings \u001b[38;5;241m=\u001b[39m create_embeddings(chunks)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Start interactive chat directly without sample question\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 167\u001b[0m, in \u001b[0;36mchat\u001b[1;34m(model, chunks, embeddings)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mAsk a question: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m question\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbye\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThank you for using the CTSE Lecture Notes Chatbot. Goodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Saranga\\Desktop\\chatbot\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Saranga\\Desktop\\chatbot\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Add these imports for PDF handling\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "# Set your API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBzS_RgPK9r-ZAWFndoDkm6TunuIpRRSlA\"\n",
    "\n",
    "# Step 1: Load documents from the 'data' folder - UPDATED to handle PDFs\n",
    "def load_documents(directory='./data'):\n",
    "    documents = []\n",
    "    \n",
    "    # Process text files\n",
    "    for file_path in glob.glob(f\"{directory}/*.txt\"):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            documents.append({\n",
    "                \"content\": content,\n",
    "                \"source\": file_path\n",
    "            })\n",
    "    \n",
    "    # Process PDF files\n",
    "    for file_path in glob.glob(f\"{directory}/*.pdf\"):\n",
    "        try:\n",
    "            content = extract_text_from_pdf(file_path)\n",
    "            documents.append({\n",
    "                \"content\": content,\n",
    "                \"source\": file_path\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {file_path}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "# New helper function to extract text from PDFs\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text() + \"\\n\\n\"\n",
    "    \n",
    "    # Clean up the text (remove extra whitespace, etc.)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Step 2: Split documents into chunks (simple version)\n",
    "def split_into_chunks(documents, chunk_size=800, overlap=150):\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        content = doc[\"content\"]\n",
    "        source = doc[\"source\"]\n",
    "        \n",
    "        # Simple sliding window approach\n",
    "        for i in range(0, len(content), chunk_size - overlap):\n",
    "            chunk_text = content[i:i + chunk_size]\n",
    "            if len(chunk_text) < 100:  # Skip very small chunks\n",
    "                continue\n",
    "            chunks.append({\n",
    "                \"content\": chunk_text,\n",
    "                \"source\": source\n",
    "            })\n",
    "    \n",
    "    print(f\"Split into {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# Step 3: Create embeddings\n",
    "def create_embeddings(chunks):\n",
    "    print(\"Loading embedding model...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embedding = model.encode(chunk[\"content\"])\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    return model, embeddings\n",
    "\n",
    "# Step 4: Simple retrieval function\n",
    "def retrieve_relevant_chunks(query, model, chunks, embeddings, k=3):\n",
    "    # Get query embedding\n",
    "    query_embedding = model.encode(query)\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "    \n",
    "    # Get top k chunks\n",
    "    top_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    return [chunks[i] for i in top_indices]\n",
    "\n",
    "# Step 5: Function to call Google Gemini API\n",
    "def ask_gemini(prompt):\n",
    "    api_key = os.environ.get(\"GOOGLE_API_KEY\", \"\")\n",
    "    \n",
    "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}\"\n",
    "    \n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"contents\": [{\n",
    "            \"parts\":[{\"text\": prompt}]\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            response_json = response.json()\n",
    "            if \"candidates\" in response_json and len(response_json[\"candidates\"]) > 0:\n",
    "                return response_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "            else:\n",
    "                return \"No valid response found in API response\"\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error calling API: {str(e)}\"\n",
    "\n",
    "# Step 6: Main QA function\n",
    "def answer_question(question, model, chunks, embeddings):\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get relevant chunks\n",
    "    relevant_chunks = retrieve_relevant_chunks(question, model, chunks, embeddings)\n",
    "    \n",
    "    # Create prompt for Gemini\n",
    "    context = \"\\n\\n\".join([chunk[\"content\"] for chunk in relevant_chunks])\n",
    "    prompt = f\"\"\"Answer the following question based ONLY on the information provided in the context below.\n",
    "    If the answer is not found in the context, say \"I don't have enough information to answer this question.\"\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    QUESTION:\n",
    "    {question}\n",
    "\n",
    "    ANSWER:\"\"\"\n",
    "    \n",
    "    # Get answer from Gemini\n",
    "    answer = ask_gemini(prompt)\n",
    "    \n",
    "    print(f\"Answer: {answer}\\n\")\n",
    "    print(\"Sources:\")\n",
    "    for i, chunk in enumerate(relevant_chunks):\n",
    "        source_file = os.path.basename(chunk[\"source\"])\n",
    "        print(f\"Source {i+1} from {source_file}:\\n{chunk['content'][:150]}...\\n\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Step 7: Improved interactive chat interface\n",
    "def chat(model, chunks, embeddings):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"  CTSE Lecture Notes Chatbot\")\n",
    "    print(\"  Type 'exit' to quit or 'new' for a new question\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nAsk a question: \")\n",
    "        \n",
    "        if question.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"\\nThank you for using the CTSE Lecture Notes Chatbot. Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        answer = answer_question(question, model, chunks, embeddings)\n",
    "        \n",
    "        # Ask if user wants to continue\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"Type your next question, 'exit' to quit, or press Enter to continue\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and process documents\n",
    "    documents = load_documents()\n",
    "    chunks = split_into_chunks(documents, chunk_size=800)\n",
    "    model, embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Start interactive chat directly without sample question\n",
    "    chat(model, chunks, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
